{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffa7503-41a6-4610-ba4c-55fc61ab6ae1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Copyright © 2023 Taeyoung Kim and Mingi Kang. All rights reserved. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce26cc9-b7b9-447f-875d-fd34c2e754e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is the solution for the homework assignment of the Machine Learning and Optimization lecture for WS2023. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997ce0c-12fb-4664-b7b2-7f39a322da1c",
   "metadata": {},
   "source": [
    "### Part 1 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fc87ad-dcef-4c95-866a-2a50fa745db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (from lecture notes)\n",
    "dim_embd = 768  # Dimension of the embeddings\n",
    "n_heads = 12    # Number of heads in the multi-head attention mechanism\n",
    "n_layers = 12   # Number of layers in the model\n",
    "dim_feedforward = 3072  # Dimension of the feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63382553-c0aa-44d0-bd11-5ed3ff0bd257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84934656"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-Head Attention parameters per layer\n",
    "# Each head has 3 metrics (Query, Key, Value) and its size is dim_embd x dim_embd/n_heads\n",
    "# Output projection matrix of size (dim_embd x dim_embd)\n",
    "mha_params_per_head = 3 * (dim_embd * dim_embd // n_heads)\n",
    "mha_params_per_layer = mha_params_per_head * n_heads\n",
    "output_proj_params = dim_embd * dim_embd\n",
    "\n",
    "# Feed-Forward Network parameters per layer\n",
    "# Two linear layers: (dim_embd x dim_feedforward) and (dim_feedforward x dim_embd)\n",
    "ffn_params_per_layer = (dim_embd * dim_feedforward) + (dim_feedforward * dim_embd)\n",
    "\n",
    "# Total non-embedding parameters per layer\n",
    "total_params_per_layer = mha_params_per_layer + output_proj_params + ffn_params_per_layer\n",
    "\n",
    "# Total non-embedding parameters in the transformer\n",
    "total_non_embedding_params = total_params_per_layer * n_layers\n",
    "total_non_embedding_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d41554-094a-4b4e-82e0-fb14791d3e18",
   "metadata": {},
   "source": [
    "### Part 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45925920-af26-4a3f-8a41-388e403b8e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87048585216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # FLOPs Calculation\n",
    "\n",
    "# # Multi-Head Attention FLOPs per layer\n",
    "# # Each head has Query, Key, Value\n",
    "# # Dimension: dim_embd x dim_embd/n_heads\n",
    "# # Output projection matrix multiplication: dim_embd x dim_embd\n",
    "# mha_flops_per_head = 3 * 2 * (dim_embd * dim_embd * (dim_embd // n_heads))  # 3 matrix, 2*ijk FLOPs each\n",
    "# output_proj_flops = 2 * (dim_embd * dim_embd * dim_embd)\n",
    "# mha_flops_per_layer = mha_flops_per_head * n_heads + output_proj_flops\n",
    "\n",
    "# # score computation: (dim_embd/n_heads x dim_embd/n_heads) for each head, per token\n",
    "# # Softmax computation as if it is relative small\n",
    "# attention_score_flops_per_head = 2 * ((dim_embd // n_heads) * (dim_embd // n_heads) * (dim_embd // n_heads))\n",
    "# attention_flops_per_layer = attention_score_flops_per_head * n_heads\n",
    "\n",
    "# # FFN FLOPs per layer\n",
    "# # 1. linear layer:dim_embd x dim_feedforward\n",
    "# # 2. linear layer: dim_feedforward x dim_embd\n",
    "# ffn_flops_per_layer = 2 * (dim_embd * dim_feedforward * dim_embd)  # 2 layers, 2*ijk FLOPs each\n",
    "\n",
    "# # Total FLOPs\n",
    "# total_flops_per_layer = mha_flops_per_layer + attention_flops_per_layer + ffn_flops_per_layer\n",
    "\n",
    "# # Total FLOPs in the transformer per token\n",
    "# # Assumued that : token is processed at a time in the forward pass\n",
    "# total_flops_per_token = total_flops_per_layer * n_layers\n",
    "# total_flops_per_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e48fe7-fb40-44cb-b9dd-fa3fb08cf58b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250123124736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate FLOPs for matrix \n",
    "# matrix A (i x j) and B (j x k)\n",
    "def calculate_flops(i, j, k):\n",
    "    return 2 * i * j * k\n",
    "\n",
    "# Multi-Head Attention FLOPs per layer\n",
    "# For Query, Key, Value matrix: A = (dim_embd, dim_embd), B = (dim_embd, dim_embd/n_heads)\n",
    "# For Output projection matrix: A = (dim_embd, dim_embd), B = (dim_embd, dim_embd)\n",
    "qkv_flops = 3 * calculate_flops(dim_embd, dim_embd, dim_embd // n_heads)  # 3 matrix: Q, K, V\n",
    "output_proj_flops = calculate_flops(dim_embd, dim_embd, dim_embd)\n",
    "mha_flops_per_layer = (qkv_flops + output_proj_flops) * n_heads\n",
    "\n",
    "# score calculation FLOP\n",
    "# A = (dim_embd/n_heads, dim_embd/n_heads), B = (dim_embd/n_heads, dim_embd/n_heads)\n",
    "attention_score_flops_per_head = calculate_flops(dim_embd // n_heads, dim_embd // n_heads, dim_embd // n_heads)\n",
    "attention_flops_per_layer = attention_score_flops_per_head * n_heads\n",
    "\n",
    "# FFN FLOPs\n",
    "# 1. lin layer: A = (dim_embd, dim_embd), B = (dim_embd, dim_feedforward)\n",
    "# 2. lin layer: A = (dim_embd, dim_feedforward), B = (dim_feedforward, dim_embd)\n",
    "ffn_flops_1 = calculate_flops(dim_embd, dim_embd, dim_feedforward)\n",
    "ffn_flops_2 = calculate_flops(dim_embd, dim_feedforward, dim_embd)\n",
    "ffn_flops_per_layer = ffn_flops_1 + ffn_flops_2\n",
    "\n",
    "# Total FLOP\n",
    "total_flops_per_layer = mha_flops_per_layer + attention_flops_per_layer + ffn_flops_per_layer\n",
    "\n",
    "# Total FLOPs in transformer /per token\n",
    "total_flops_per_token = total_flops_per_layer * n_layers\n",
    "total_flops_per_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09761d6-81da-4a12-9f0f-3afca4fadd0d",
   "metadata": {},
   "source": [
    "### Part 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c2920e-d904-4fe2-8b97-b7123644fde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509607936"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total non-embedding parameters from Question 1\n",
    "total_non_embedding_params = 84934656  # From previous calculation\n",
    "\n",
    "# Estimated computational cost\n",
    "# Forward: äquivalent to  number of non-embedding parameters N\n",
    "# Backward: äquivalten to double the forward pass\n",
    "# Total passes: 3N\n",
    "# each operation in matrx  as a separate operation: roughly 6N\n",
    "estimated_training_cost_per_token = 6 * total_non_embedding_params\n",
    "estimated_training_cost_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16833c32-0859-49ba-9a16-791fec6a93d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8c9c8-c0ef-4627-b775-1f2cc97ec45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e9236-217e-4136-9259-684bc55c3121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c2087-eba7-4c89-a65c-1a44db3e9368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6278f-cb80-4d30-bbec-1544ab6c5bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80544288-45f4-4914-8024-e95c5a66a53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d96ca-8d8c-47dc-8c9a-cbc254f7e511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81f58b-b2d2-4410-8552-ee82940d86cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15dfa3-ac17-40ad-8c4d-a366edd575c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3a468-3042-4f07-94cf-780318c5f903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b7174-8e27-4963-9644-b44c06e71fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef81a7-8e9e-42dc-9083-7728b950843b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545d646-2212-4c5e-9235-e97bebc8bc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c1436-bfc0-4567-8bc4-4419457d162c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39a27a-8a44-4d6e-abd1-3643eb36ce1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13955eb6-c4e1-42ca-abe8-7b8028c16873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77325e-e084-4ec7-88d4-0e203a307400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa156b-385a-47aa-a6d5-e0060c88cfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed64e90-f93c-483a-9ee5-80fd663b308a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a31dc-3edd-48d1-8911-324ef9d57d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66652585-cec2-41c6-8328-b980c4afe02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d301c-22d3-4c2c-aeba-216ade527631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad16b40-a3f9-4a62-84b0-822e0f5b269b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
